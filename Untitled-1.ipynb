{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch\n",
    "from einops import rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<class 'torchvision.transforms._presets.ImageClassification'>, crop_size=380, resize_size=384, interpolation=<InterpolationMode.BICUBIC: 'bicubic'>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.models.EfficientNet_B4_Weights.IMAGENET1K_V1.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.efficientnet_b4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1792, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.efficientnet_b4()\n",
    "modules=list(model.children())[:-1]\n",
    "modules\n",
    "model=nn.Sequential(*modules)\n",
    "# Random image of 380x380x3\n",
    "x = torch.randn(2, 3, 380, 380)\n",
    "\n",
    "# model(x).view(-1, 1792, 1).size()\n",
    "torch.flatten(model(x), start_dim=1).view(-1, 1792, 1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_classes, depth_transformer, heads_transformer, dim_fc_transformer):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        full_cnn = torchvision.models.convnext_tiny(weights=\"DEFAULT\")   \n",
    "        modules=list(full_cnn.children())[:-2]\n",
    "        self.feature_extractor=nn.Sequential(*modules)\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.dim_features_feature_extractor = 768\n",
    "        self.n_features_feature_extractor = 49\n",
    "        self.text_features = 300\n",
    "        # Dimension in which the images and text are embedded\n",
    "        self.dim = 350\n",
    "\n",
    "        # Embed for the text and image features\n",
    "        self.cnn_features_embed = nn.Linear(self.n_features_feature_extractor, self.dim)\n",
    "        self.text_features_embed = nn.Linear(self.text_features, self.dim)\n",
    "\n",
    "        # Positional embedding for the image features\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.dim_features_feature_extractor + 1, self.dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.dim))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        # The Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.dim, nhead=heads_transformer, dim_feedforward=dim_fc_transformer, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth_transformer)\n",
    "\n",
    "        # Classification fc\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.dim, 256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, txt):\n",
    "        batch_size = img.shape[0]\n",
    "\n",
    "        image_features = self.feature_extractor(img)\n",
    "        print(image_features.size())\n",
    "        image_features = image_features.reshape(batch_size, self.n_features_feature_extractor, self.dim_features_feature_extractor).permute(0, 2, 1)\n",
    "        image_features = self.cnn_features_embed(image_features) \n",
    "\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, image_features), dim=1)\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        text_features = self.text_features_embed(txt)\n",
    "        x = torch.cat((x, text_features), dim=1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x[:, 0]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4002, -0.0557, -0.4475,  0.4087, -0.2469, -0.7703,  0.1720, -0.5460,\n",
       "         -0.3017, -0.0252, -0.1599,  0.0577, -0.1675,  0.1278,  0.2412,  0.2234,\n",
       "          0.2146, -0.0628, -0.2319,  0.0391, -0.2654, -0.2683,  0.0431,  0.3022,\n",
       "         -0.3157,  0.0569,  0.1610, -0.2194],\n",
       "        [ 0.4653,  0.2245, -0.3757,  0.4913,  0.2200, -0.0430,  0.0553, -0.5782,\n",
       "         -0.1992,  0.2546,  0.1512, -0.1181, -0.6191,  0.4201,  0.0352,  0.0398,\n",
       "         -0.1506,  0.1418,  0.0696, -0.2588,  0.2420, -0.3117, -0.2428,  0.1430,\n",
       "         -0.5008, -0.1803,  0.1910,  0.0842]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(num_classes=28, depth_transformer=4, heads_transformer=5, dim_fc_transformer=300)\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "y = torch.randn(2, 20, 300)\n",
    "model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConTextTransformer(num_classes=28, depth=4, heads=5, dim=300, mlp_dim=300)\n",
    "x = torch.randn(2, 3, 240, 240)\n",
    "y = torch.randn(2, 20, 300)\n",
    "model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConTextTransformer(nn.Module):\n",
    "    def __init__(self, *, num_classes, dim, depth, heads, mlp_dim, channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Visual feature extractor\n",
    "        resnet50 = torchvision.models.resnet50(weights=\"DEFAULT\")\n",
    "        modules=list(resnet50.children())[:-2]\n",
    "        self.resnet50=nn.Sequential(*modules)\n",
    "        for param in self.resnet50.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.num_cnn_features = 64  # 8x8\n",
    "        self.dim_cnn_features = 2048\n",
    "        self.dim_fasttext_features = 300\n",
    "\n",
    "        # Embeddings for the visual and textual features\n",
    "        self.cnn_feature_to_embedding = nn.Linear(self.dim_cnn_features, dim)\n",
    "        self.fasttext_feature_to_embedding = nn.Linear(self.dim_fasttext_features, dim)\n",
    "\n",
    "        # Learnable position embeddings (for the visual features) and CLS token\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_cnn_features + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "        # The Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, batch_first=True)\n",
    "        encoder_norm = nn.LayerNorm(dim)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        # Classification Head (MLP)\n",
    "        self.to_cls_token = nn.Identity()\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(mlp_dim, mlp_dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(mlp_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, txt, mask=None):\n",
    "        x = self.resnet50(img)\n",
    "        x = rearrange(x, 'b d h w -> b (h w) d') # this makes a sequence of 64 visual features\n",
    "        print(x.size())\n",
    "        x = self.cnn_feature_to_embedding(x)\n",
    "        print(x.size())\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
    "        print(cls_tokens.size())\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        print(x.size())\n",
    "        print(self.pos_embedding.size())\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        x2 = self.fasttext_feature_to_embedding(txt.float())\n",
    "        x = torch.cat((x,x2), dim=1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        return self.mlp_head(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataloders\n",
    "\n",
    "\"\"\"\n",
    "model, criterion, optimizer, data_transforms_train = make2(config)\n",
    "data_path = \"C:/Users/Joan/Desktop/Deep_Learning_project/features/data/ImageSets/0\"\n",
    "img_dir = \"C:/Users/Joan/Desktop/Deep_Learning_project/features/data/JPEGImages\"\n",
    "anotation_path= r\"C:\\Users\\Joan\\Desktop\\Deep_Learning_project\\dlnn-project_ia-group_15\\anotations.pkl\"\n",
    "train_img_names, y_train, test_img_names, y_test, val_img_names, y_val = load_labels_and_split(data_path)\n",
    "ocr_data = pd.read_pickle(anotation_path)\n",
    "train_dataset = Dataset_ConText(img_dir, train_img_names, y_train, ocr_data, transform=data_transforms_train)\n",
    "train_loader = make_loader(train_dataset, config.batch_size)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions\n",
    "\"\"\"\n",
    "def make(config, train=True, device=\"cuda\"):\n",
    "    # Make the data and model\n",
    "    data_path = \"C:/Users/Joan/Desktop/Deep_Learning_project/features/data/\"\n",
    "    anotation_path= r\"C:\\Users\\Joan\\Desktop\\Deep_Learning_project\\dlnn-project_ia-group_15\\anotations.pkl\"\n",
    "    input_size = 256\n",
    "    if train:\n",
    "        data_transforms_train = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.RandomResizedCrop(input_size),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        train_df, val_df = make_dataframe(data_path, anotation_path, train=train)\n",
    "        train_dataset = Dataset_ConText(train_df, data_transforms_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
    "        val_dataset = Dataset_ConText(val_df, data_transforms_train)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "        # Make the model\n",
    "        model = ConTextTransformer(num_classes=config.classes, channels=3, dim=256, depth=2, heads=4, mlp_dim=512).to(device)\n",
    "\n",
    "        # Make the loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "        return model, train_loader, val_loader, criterion, optimizer\n",
    "    else:\n",
    "        data_transforms_test = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(input_size),\n",
    "            torchvision.transforms.CenterCrop(input_size),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        test_df = make_dataframe(data_path, anotation_path, train=train)\n",
    "        test_dataset = Dataset_ConText(test_df, data_transforms_train)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
    "        return  test_loader\n",
    "\n",
    "# Adds the columns of two dataframes\n",
    "def merge_data(imagesAndLabels, ocr_data):\n",
    "    data = pd.concat([imagesAndLabels, ocr_data], axis=1, join=\"inner\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# Call this function to get the dataframes of the data, if train is True, it will return the train and validation dataframes,\n",
    "#  if not, it will return the test dataframe\n",
    "def make_dataframe(data_dir, anotation_path, train=True):\n",
    "    sets_dir = data_dir + \"/ImageSets/0\"\n",
    "    train_img_names, y_train, test_img_names, y_test, val_img_names, y_val = load_labels_and_split(sets_dir)\n",
    "    ocr_data = pd.read_pickle(anotation_path)\n",
    "    if train:\n",
    "        train_data = load_images(train_img_names, y_train, data_dir)\n",
    "        val_data = load_images(val_img_names, y_val, data_dir)\n",
    "        train_data = merge_data(train_data, ocr_data)\n",
    "        val_data = merge_data(val_data, ocr_data)\n",
    "        return train_data.iloc[:int(len(train_data.index)/2), :], val_data\n",
    "    else:\n",
    "        test_data = load_images(test_img_names, y_test, data_dir)\n",
    "        test_data = merge_data(test_data, ocr_data)\n",
    "        return test_data\n",
    "\n",
    "\n",
    "# Loads the images and creates a dataframe with the correpondent labels\n",
    "def load_images(img_names, labels, data_dir):\n",
    "    img_dir = data_dir + \"JPEGImages\"\n",
    "\n",
    "    list_img = []\n",
    "    for img_name in img_names:\n",
    "        img = Image.open(os.path.join(img_dir, img_name)).convert('RGB')\n",
    "        list_img.append(torch.tensor(img, dtype=torch.ByteTensor).repeat(3, 1, 1))\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    data[\"img\"] = list_img\n",
    "    data[\"label\"] = labels\n",
    "    data[\"name\"] = img_names\n",
    "    data.set_index(\"name\", inplace=True)\n",
    "    data[\"label\"] = data[\"label\"].astype(int)\n",
    "\n",
    "    return data\n",
    "    \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
