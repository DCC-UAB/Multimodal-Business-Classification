{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "# fasttext.util.download_model('en') \n",
    "w2v = fasttext.load_model(r'C:\\Users\\Joan\\Desktop\\Deep_Learning_project\\cc.en.300.bin')\n",
    "w2v.get_word_vector(\"hello\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " 'the',\n",
       " '.',\n",
       " 'and',\n",
       " 'to',\n",
       " 'of',\n",
       " 'a',\n",
       " '</s>',\n",
       " 'in',\n",
       " 'is',\n",
       " ':',\n",
       " 'I',\n",
       " 'for',\n",
       " 'that',\n",
       " ')',\n",
       " '\"',\n",
       " '(',\n",
       " 'on',\n",
       " 'with',\n",
       " 'it',\n",
       " 'you',\n",
       " 'The',\n",
       " 'was',\n",
       " 'as',\n",
       " 'are',\n",
       " 'at',\n",
       " '/',\n",
       " '’',\n",
       " 'be',\n",
       " 'by',\n",
       " \"'s\",\n",
       " 'this',\n",
       " 'have',\n",
       " 'from',\n",
       " 'or',\n",
       " '!',\n",
       " 'not',\n",
       " 'your',\n",
       " 'an',\n",
       " \"'\",\n",
       " 'but',\n",
       " '?',\n",
       " 'can',\n",
       " '-',\n",
       " 'will',\n",
       " 's',\n",
       " 'my',\n",
       " 'has',\n",
       " 'all',\n",
       " 'we',\n",
       " 'they',\n",
       " 'he',\n",
       " 'his',\n",
       " 'more',\n",
       " 'one',\n",
       " 'about',\n",
       " 'their',\n",
       " \"'t\",\n",
       " 'so',\n",
       " 'which',\n",
       " 'It',\n",
       " 'out',\n",
       " 'up',\n",
       " '...',\n",
       " 'were',\n",
       " 'had',\n",
       " 'who',\n",
       " 'like',\n",
       " ';',\n",
       " '“',\n",
       " 'our',\n",
       " 'would',\n",
       " '”',\n",
       " 'time',\n",
       " 'been',\n",
       " 'if',\n",
       " 'also',\n",
       " 'just',\n",
       " 'when',\n",
       " 'her',\n",
       " 'This',\n",
       " 'me',\n",
       " 'there',\n",
       " 'do',\n",
       " 'what',\n",
       " 'some',\n",
       " 'other',\n",
       " 'In',\n",
       " 'them',\n",
       " '–',\n",
       " '1',\n",
       " 'get',\n",
       " 'new',\n",
       " 'into',\n",
       " '&',\n",
       " 'We',\n",
       " 'than',\n",
       " 'A',\n",
       " 'no',\n",
       " 'only',\n",
       " 'first',\n",
       " 'any',\n",
       " 'its',\n",
       " 'people',\n",
       " '2',\n",
       " '$',\n",
       " 'very',\n",
       " 't',\n",
       " 'over',\n",
       " 'she',\n",
       " '%',\n",
       " 'how',\n",
       " 'make',\n",
       " 'You',\n",
       " 'said',\n",
       " 'He',\n",
       " 'two',\n",
       " 'may',\n",
       " 'know',\n",
       " 'then',\n",
       " 'see',\n",
       " 'after',\n",
       " 'most',\n",
       " 'good',\n",
       " 'years',\n",
       " 'If',\n",
       " 'these',\n",
       " 'now',\n",
       " '3',\n",
       " 'use',\n",
       " 'because',\n",
       " 'well',\n",
       " 'work',\n",
       " 'could',\n",
       " 'us',\n",
       " 'don',\n",
       " 'way',\n",
       " 'much',\n",
       " 'back',\n",
       " 'many',\n",
       " 'think',\n",
       " 'where',\n",
       " 'even',\n",
       " 'him',\n",
       " 'through',\n",
       " 'am',\n",
       " '10',\n",
       " '|',\n",
       " 'here',\n",
       " '#',\n",
       " 'made',\n",
       " 'year',\n",
       " 'should',\n",
       " '*',\n",
       " 'really',\n",
       " 'being',\n",
       " 'such',\n",
       " 'need',\n",
       " 'great',\n",
       " 'And',\n",
       " ']',\n",
       " '4',\n",
       " '[',\n",
       " '5',\n",
       " 'day',\n",
       " 'before',\n",
       " 'want',\n",
       " 'used',\n",
       " 'go',\n",
       " 'those',\n",
       " '…',\n",
       " 'But',\n",
       " 'right',\n",
       " \"'m\",\n",
       " 'take',\n",
       " '—',\n",
       " 'May',\n",
       " 'still',\n",
       " 'last',\n",
       " 'off',\n",
       " 'too',\n",
       " 'New',\n",
       " 'going',\n",
       " 'best',\n",
       " 'find',\n",
       " 'love',\n",
       " 'did',\n",
       " 'while',\n",
       " 'home',\n",
       " 'There',\n",
       " 'They',\n",
       " 'same',\n",
       " 'around',\n",
       " 'help',\n",
       " 'down',\n",
       " 'information',\n",
       " 'UTC',\n",
       " 'place',\n",
       " 'i',\n",
       " '2017',\n",
       " 'For',\n",
       " 'little',\n",
       " 'life',\n",
       " 'between',\n",
       " 'each',\n",
       " 'own',\n",
       " 'both',\n",
       " '12',\n",
       " '6',\n",
       " 'world',\n",
       " 'part',\n",
       " 'few',\n",
       " '8',\n",
       " '7',\n",
       " 'talk',\n",
       " 'As',\n",
       " 'look',\n",
       " '2012',\n",
       " 'things',\n",
       " '11',\n",
       " 'say',\n",
       " 'does',\n",
       " 'every',\n",
       " 'something',\n",
       " '2013',\n",
       " 'during',\n",
       " 'got',\n",
       " 'So',\n",
       " \"'ve\",\n",
       " 'What',\n",
       " 'since',\n",
       " 'found',\n",
       " 'long',\n",
       " 'different',\n",
       " 'says',\n",
       " '>',\n",
       " 'never',\n",
       " 'another',\n",
       " '�',\n",
       " 'Ã',\n",
       " 'better',\n",
       " '2016',\n",
       " 'using',\n",
       " '+',\n",
       " 'free',\n",
       " '20',\n",
       " 'under',\n",
       " 'three',\n",
       " 'family',\n",
       " 'She',\n",
       " 'including',\n",
       " 'That',\n",
       " 'always',\n",
       " '\\\\',\n",
       " 'next',\n",
       " 'come',\n",
       " 'without',\n",
       " 'My',\n",
       " 'again',\n",
       " '9',\n",
       " 'game',\n",
       " \"'re\",\n",
       " '15',\n",
       " '2011',\n",
       " 'When',\n",
       " 'days',\n",
       " 'set',\n",
       " '30',\n",
       " 'All',\n",
       " 'number',\n",
       " '2014',\n",
       " 'end',\n",
       " 'lot',\n",
       " 'business',\n",
       " 'sure',\n",
       " 'system',\n",
       " 'book',\n",
       " '2015',\n",
       " 'against',\n",
       " 'high',\n",
       " '=',\n",
       " '2010',\n",
       " 'must',\n",
       " 'available',\n",
       " 'To',\n",
       " 'might',\n",
       " 'show',\n",
       " 'area',\n",
       " 'Â',\n",
       " 'week',\n",
       " '00',\n",
       " 'away',\n",
       " 'team',\n",
       " 'March',\n",
       " 'name',\n",
       " 'until',\n",
       " 'April',\n",
       " 'give',\n",
       " 'thing',\n",
       " 'read',\n",
       " 'put',\n",
       " \"'ll\",\n",
       " 'On',\n",
       " 'small',\n",
       " 'school',\n",
       " 'feel',\n",
       " 'second',\n",
       " 'company',\n",
       " 'June',\n",
       " 'old',\n",
       " 'didn',\n",
       " 'page',\n",
       " 'keep',\n",
       " 'top',\n",
       " 'why',\n",
       " 'January',\n",
       " 'site',\n",
       " '18',\n",
       " 'post',\n",
       " 'today',\n",
       " '16',\n",
       " 'within',\n",
       " '0',\n",
       " 'service',\n",
       " 'having',\n",
       " 'looking',\n",
       " 'American',\n",
       " 'point',\n",
       " 'data',\n",
       " 'though',\n",
       " '½',\n",
       " 'July',\n",
       " 'University',\n",
       " 'full',\n",
       " '14',\n",
       " 'm',\n",
       " 'able',\n",
       " 'left',\n",
       " 'With',\n",
       " 'support',\n",
       " 'United',\n",
       " '2009',\n",
       " '13',\n",
       " 'experience',\n",
       " 'room',\n",
       " '¿',\n",
       " 'water',\n",
       " 'state',\n",
       " '‘',\n",
       " 'big',\n",
       " 'order',\n",
       " 'October',\n",
       " 'No',\n",
       " 'article',\n",
       " 'case',\n",
       " 'making',\n",
       " 'enough',\n",
       " 'ago',\n",
       " 'house',\n",
       " 'September',\n",
       " '17',\n",
       " 'children',\n",
       " 'One',\n",
       " 'local',\n",
       " 'December',\n",
       " 'start',\n",
       " 'times',\n",
       " 'February',\n",
       " 'called',\n",
       " 'August',\n",
       " '25',\n",
       " 'November',\n",
       " 'ever',\n",
       " 'More',\n",
       " '2008',\n",
       " 'done',\n",
       " 'change',\n",
       " 'play',\n",
       " 'group',\n",
       " 'working',\n",
       " 'based',\n",
       " 'online',\n",
       " 'http',\n",
       " 'course',\n",
       " 'least',\n",
       " 'doesn',\n",
       " 'money',\n",
       " 'won',\n",
       " 'man',\n",
       " 'less',\n",
       " '@',\n",
       " 'important',\n",
       " 'After',\n",
       " '24',\n",
       " 'How',\n",
       " '22',\n",
       " 'try',\n",
       " 'getting',\n",
       " 'thought',\n",
       " 'public',\n",
       " 'actually',\n",
       " 'already',\n",
       " 'night',\n",
       " 'person',\n",
       " 're',\n",
       " '21',\n",
       " 'went',\n",
       " '..',\n",
       " 'story',\n",
       " 'several',\n",
       " '--',\n",
       " 'later',\n",
       " 'makes',\n",
       " 'side',\n",
       " 'list',\n",
       " 'following',\n",
       " '19',\n",
       " 'came',\n",
       " 'By',\n",
       " 'doing',\n",
       " 'power',\n",
       " 'large',\n",
       " 'season',\n",
       " 'provide',\n",
       " 'website',\n",
       " 'bit',\n",
       " 'far',\n",
       " 'real',\n",
       " 'known',\n",
       " 'took',\n",
       " 'PM',\n",
       " 'city',\n",
       " '23',\n",
       " 'let',\n",
       " 'At',\n",
       " 'along',\n",
       " 'together',\n",
       " 'per',\n",
       " '}',\n",
       " 'often',\n",
       " 'These',\n",
       " 'music',\n",
       " 'hard',\n",
       " 'God',\n",
       " 'share',\n",
       " 'Your',\n",
       " 'Our',\n",
       " 'line',\n",
       " 'process',\n",
       " 'care',\n",
       " 'others',\n",
       " 'open',\n",
       " 'John',\n",
       " 'car',\n",
       " 'services',\n",
       " 'include',\n",
       " 'food',\n",
       " 'started',\n",
       " 'easy',\n",
       " 'country',\n",
       " 'months',\n",
       " 'fact',\n",
       " 'women',\n",
       " 'yet',\n",
       " 'students',\n",
       " '2007',\n",
       " 'someone',\n",
       " 'once',\n",
       " 'live',\n",
       " 'problem',\n",
       " 'run',\n",
       " 'video',\n",
       " 've',\n",
       " 'become',\n",
       " 'government',\n",
       " 'everything',\n",
       " 'series',\n",
       " 'However',\n",
       " 'anything',\n",
       " 'four',\n",
       " 'pm',\n",
       " 'means',\n",
       " 'stay',\n",
       " '100',\n",
       " 'early',\n",
       " 'possible',\n",
       " 'past',\n",
       " 'design',\n",
       " 'quality',\n",
       " 'City',\n",
       " 'seen',\n",
       " 'States',\n",
       " 'given',\n",
       " 'pretty',\n",
       " 'State',\n",
       " 'blog',\n",
       " 'U.S.',\n",
       " \"'d\",\n",
       " 'film',\n",
       " 'nice',\n",
       " 'history',\n",
       " 'job',\n",
       " 'call',\n",
       " 'Now',\n",
       " 'kind',\n",
       " 'believe',\n",
       " 'community',\n",
       " 'needs',\n",
       " 'level',\n",
       " 'program',\n",
       " 'body',\n",
       " 'view',\n",
       " 'friends',\n",
       " 'control',\n",
       " 'comes',\n",
       " 'York',\n",
       " 'products',\n",
       " 'form',\n",
       " 'above',\n",
       " '26',\n",
       " 'School',\n",
       " 'health',\n",
       " 'minutes',\n",
       " 'probably',\n",
       " 'World',\n",
       " 'hours',\n",
       " 'fun',\n",
       " 'National',\n",
       " 'US',\n",
       " 'either',\n",
       " 'County',\n",
       " 'head',\n",
       " '28',\n",
       " 'example',\n",
       " 'product',\n",
       " 'please',\n",
       " 'close',\n",
       " 'beautiful',\n",
       " 'market',\n",
       " 'across',\n",
       " 'whole',\n",
       " 'offer',\n",
       " 'quite',\n",
       " 'trying',\n",
       " 'price',\n",
       " 'told',\n",
       " 'idea',\n",
       " 'members',\n",
       " 'light',\n",
       " '27',\n",
       " 'million',\n",
       " 'single',\n",
       " 'His',\n",
       " 'perfect',\n",
       " 'Please',\n",
       " 'hope',\n",
       " 'access',\n",
       " 'research',\n",
       " 'due',\n",
       " '2006',\n",
       " 'tell',\n",
       " 'seems',\n",
       " 'South',\n",
       " 'added',\n",
       " 'almost',\n",
       " 'current',\n",
       " 'short',\n",
       " 'bad',\n",
       " 'Not',\n",
       " '50',\n",
       " 'games',\n",
       " 'project',\n",
       " 'review',\n",
       " 'hand',\n",
       " 'email',\n",
       " 'near',\n",
       " 'below',\n",
       " 'men',\n",
       " 'wanted',\n",
       " '29',\n",
       " 'create',\n",
       " 'nothing',\n",
       " 'question',\n",
       " 'de',\n",
       " 'special',\n",
       " 'everyone',\n",
       " 'content',\n",
       " 'Posted',\n",
       " 'future',\n",
       " 'rather',\n",
       " 'check',\n",
       " 'taking',\n",
       " 'Thanks',\n",
       " 'works',\n",
       " 'From',\n",
       " 'month',\n",
       " 'front',\n",
       " 'law',\n",
       " 'space',\n",
       " 'news',\n",
       " 'Here',\n",
       " 'reason',\n",
       " 'add',\n",
       " 'isn',\n",
       " 'anyone',\n",
       " 'report',\n",
       " 'whether',\n",
       " 'major',\n",
       " 'main',\n",
       " 'young',\n",
       " 'development',\n",
       " 'North',\n",
       " 'event',\n",
       " '•',\n",
       " 'results',\n",
       " 'personal',\n",
       " 'especially',\n",
       " 'mean',\n",
       " 'issue',\n",
       " 'five',\n",
       " 'social',\n",
       " 'original',\n",
       " 'age',\n",
       " '....',\n",
       " 'born',\n",
       " 'taken',\n",
       " 'living',\n",
       " 'buy',\n",
       " 'd',\n",
       " 'll',\n",
       " 'Reply',\n",
       " 'reading',\n",
       " 'English',\n",
       " 'mind',\n",
       " 'study',\n",
       " 'building',\n",
       " 'result',\n",
       " 'type',\n",
       " 'looks',\n",
       " 'located',\n",
       " 'features',\n",
       " 'words',\n",
       " 'party',\n",
       " 'needed',\n",
       " 'size',\n",
       " 'plan',\n",
       " 'issues',\n",
       " 'else',\n",
       " 'search',\n",
       " 'black',\n",
       " 'version',\n",
       " 'News',\n",
       " 'included',\n",
       " 'Read',\n",
       " 'books',\n",
       " 'House',\n",
       " 'provided',\n",
       " 'Just',\n",
       " 'visit',\n",
       " 'couple',\n",
       " 'white',\n",
       " 'present',\n",
       " 'link',\n",
       " 'pay',\n",
       " 'coming',\n",
       " 'Some',\n",
       " 'face',\n",
       " 'kids',\n",
       " 'questions',\n",
       " 'friend',\n",
       " 'understand',\n",
       " 'however',\n",
       " 'human',\n",
       " 'became',\n",
       " 'ï',\n",
       " 'value',\n",
       " 'move',\n",
       " 'further',\n",
       " 'member',\n",
       " 'among',\n",
       " 'half',\n",
       " 'weeks',\n",
       " 'third',\n",
       " 'media',\n",
       " 'true',\n",
       " 'America',\n",
       " '»',\n",
       " 'soon',\n",
       " 'range',\n",
       " 'received',\n",
       " 'While',\n",
       " 'points',\n",
       " 'held',\n",
       " 'image',\n",
       " 'title',\n",
       " 'property',\n",
       " 'town',\n",
       " 'shows',\n",
       " 'offers',\n",
       " 'movie',\n",
       " 'happy',\n",
       " 'asked',\n",
       " 'outside',\n",
       " '£',\n",
       " 'various',\n",
       " 'writing',\n",
       " 'etc',\n",
       " 'class',\n",
       " '31',\n",
       " 'Is',\n",
       " 'contact',\n",
       " 'TV',\n",
       " 'upon',\n",
       " 'An',\n",
       " 'style',\n",
       " 'phone',\n",
       " 'stop',\n",
       " 'created',\n",
       " 'location',\n",
       " 'Click',\n",
       " 'wrote',\n",
       " 'running',\n",
       " 'art',\n",
       " 'low',\n",
       " 'child',\n",
       " 'heart',\n",
       " 'date',\n",
       " 'Do',\n",
       " 'matter',\n",
       " 'simple',\n",
       " 'address',\n",
       " 'played',\n",
       " 'similar',\n",
       " 'simply',\n",
       " 'office',\n",
       " 'enjoy',\n",
       " 'myself',\n",
       " 'saw',\n",
       " 'behind',\n",
       " 'complete',\n",
       " 'Center',\n",
       " 'card',\n",
       " 'death',\n",
       " 'problems',\n",
       " 'cannot',\n",
       " 'performance',\n",
       " 'Also',\n",
       " 'turn',\n",
       " 'action',\n",
       " 'learn',\n",
       " 'via',\n",
       " 'general',\n",
       " 'deal',\n",
       " 'includes',\n",
       " 'cost',\n",
       " 'bring',\n",
       " 'written',\n",
       " 'takes',\n",
       " 'window',\n",
       " 'Of',\n",
       " 'former',\n",
       " 'First',\n",
       " 'air',\n",
       " 'leave',\n",
       " 'likely',\n",
       " 'field',\n",
       " 'lost',\n",
       " 'areas',\n",
       " 'private',\n",
       " 'industry',\n",
       " 'store',\n",
       " 'usually',\n",
       " 'late',\n",
       " '2005',\n",
       " 'lead',\n",
       " 'Then',\n",
       " '40',\n",
       " 'clear',\n",
       " 'win',\n",
       " 'total',\n",
       " 'West',\n",
       " 'common',\n",
       " 'morning',\n",
       " 'Home',\n",
       " 'events',\n",
       " 'Day',\n",
       " 'released',\n",
       " 'Google',\n",
       " 'position',\n",
       " 'return',\n",
       " 'required',\n",
       " 'subject',\n",
       " 'account',\n",
       " 'ask',\n",
       " 'gets',\n",
       " 'comment',\n",
       " 'companies',\n",
       " 'provides',\n",
       " 'published',\n",
       " 'energy',\n",
       " 'currently',\n",
       " 'clean',\n",
       " 'President',\n",
       " 'instead',\n",
       " 'comments',\n",
       " 'final',\n",
       " 'longer',\n",
       " 'training',\n",
       " 'playing',\n",
       " '¯',\n",
       " 'period',\n",
       " 'interest',\n",
       " 'word',\n",
       " 'amount',\n",
       " 'source',\n",
       " 'rest',\n",
       " 'record',\n",
       " 'worked',\n",
       " 'Great',\n",
       " 'model',\n",
       " 'addition',\n",
       " 'technology',\n",
       " 'wasn',\n",
       " '<',\n",
       " 'web',\n",
       " 'details',\n",
       " 'role',\n",
       " 'College',\n",
       " 'goes',\n",
       " 'Well',\n",
       " 'Park',\n",
       " 'Free',\n",
       " 'began',\n",
       " 'professional',\n",
       " 'changes',\n",
       " 'strong',\n",
       " 'meet',\n",
       " 'watch',\n",
       " 'hotel',\n",
       " 'certain',\n",
       " 'saying',\n",
       " 'sense',\n",
       " 'forward',\n",
       " 'gave',\n",
       " 'color',\n",
       " 'recently',\n",
       " 'inside',\n",
       " 'wrong',\n",
       " 'album',\n",
       " 'staff',\n",
       " 'designed',\n",
       " 'paper',\n",
       " 'amazing',\n",
       " 'drive',\n",
       " 'woman',\n",
       " 'hit',\n",
       " 'user',\n",
       " 'rights',\n",
       " 'decided',\n",
       " 'recent',\n",
       " 'key',\n",
       " 'continue',\n",
       " 'rate',\n",
       " 'posted',\n",
       " 'itself',\n",
       " 'code',\n",
       " 'favorite',\n",
       " 'seem',\n",
       " 'International',\n",
       " 'political',\n",
       " 'View',\n",
       " 'Thank',\n",
       " 'maybe',\n",
       " 'percent',\n",
       " 'remember',\n",
       " 'Why',\n",
       " '~',\n",
       " 'London',\n",
       " 'test',\n",
       " 'stuff',\n",
       " 'write',\n",
       " 'computer',\n",
       " 'cause',\n",
       " 'cut',\n",
       " 'according',\n",
       " 'built',\n",
       " 'player',\n",
       " 'popular',\n",
       " 'High',\n",
       " 'Most',\n",
       " '01',\n",
       " 'management',\n",
       " 'interesting',\n",
       " 'son',\n",
       " 'ways',\n",
       " 'education',\n",
       " 'sound',\n",
       " 'summer',\n",
       " 'ready',\n",
       " 'natural',\n",
       " 'national',\n",
       " 'software',\n",
       " '·',\n",
       " 'David',\n",
       " 'tried',\n",
       " 'yourself',\n",
       " 'players',\n",
       " 'hot',\n",
       " 'higher',\n",
       " 'policy',\n",
       " 'related',\n",
       " 'Don',\n",
       " 'miles',\n",
       " 'entire',\n",
       " 'specific',\n",
       " 'wonderful',\n",
       " 'Yes',\n",
       " 'felt',\n",
       " 'section',\n",
       " 'British',\n",
       " 'thinking',\n",
       " 'x',\n",
       " 'release',\n",
       " 'song',\n",
       " 'San',\n",
       " 'six',\n",
       " 'heard',\n",
       " 'particular',\n",
       " 'users',\n",
       " 'War',\n",
       " 'average',\n",
       " 'hear',\n",
       " 'Best',\n",
       " 'themselves',\n",
       " 'security',\n",
       " 'file',\n",
       " 'https',\n",
       " 'ones',\n",
       " '05',\n",
       " 'oil',\n",
       " 'Dr.',\n",
       " 'Facebook',\n",
       " 'See',\n",
       " 'California',\n",
       " 'cover',\n",
       " 'allow',\n",
       " 'center',\n",
       " 'Friday',\n",
       " 'terms',\n",
       " 'road',\n",
       " 'receive',\n",
       " 'board',\n",
       " 'war',\n",
       " 'material',\n",
       " 'wife',\n",
       " 'lives',\n",
       " 'sometimes',\n",
       " 'parts',\n",
       " 'UK',\n",
       " 'individual',\n",
       " 'chance',\n",
       " 'Health',\n",
       " 'items',\n",
       " 'answer',\n",
       " 'girl',\n",
       " 'himself',\n",
       " 'India',\n",
       " 'Street',\n",
       " 'walk',\n",
       " 'definitely',\n",
       " 'C',\n",
       " 'production',\n",
       " 'worth',\n",
       " 'systems',\n",
       " 'character',\n",
       " 'throughout',\n",
       " 'reviews',\n",
       " 'giving',\n",
       " 'career',\n",
       " 'increase',\n",
       " 'unique',\n",
       " 'mother',\n",
       " 'completely',\n",
       " 'picture',\n",
       " 'although',\n",
       " 'St.',\n",
       " 'additional',\n",
       " 'customers',\n",
       " 'sex',\n",
       " 'choose',\n",
       " 'parents',\n",
       " '09',\n",
       " 'involved',\n",
       " 'medical',\n",
       " 'follow',\n",
       " 'piece',\n",
       " 'articles',\n",
       " 'red',\n",
       " 'Washington',\n",
       " 'band',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " 'the',\n",
       " '.',\n",
       " 'and',\n",
       " 'to',\n",
       " 'of',\n",
       " 'a',\n",
       " '</s>',\n",
       " 'in',\n",
       " 'is',\n",
       " ':',\n",
       " 'I',\n",
       " 'for',\n",
       " 'that',\n",
       " ')',\n",
       " '\"',\n",
       " '(',\n",
       " 'on',\n",
       " 'with',\n",
       " 'it',\n",
       " 'you',\n",
       " 'The',\n",
       " 'was',\n",
       " 'as',\n",
       " 'are',\n",
       " 'at',\n",
       " '/',\n",
       " '’',\n",
       " 'be',\n",
       " 'by',\n",
       " \"'s\",\n",
       " 'this',\n",
       " 'have',\n",
       " 'from',\n",
       " 'or',\n",
       " '!',\n",
       " 'not',\n",
       " 'your',\n",
       " 'an',\n",
       " \"'\",\n",
       " 'but',\n",
       " '?',\n",
       " 'can',\n",
       " '-',\n",
       " 'will',\n",
       " 's',\n",
       " 'my',\n",
       " 'has',\n",
       " 'all',\n",
       " 'we',\n",
       " 'they',\n",
       " 'he',\n",
       " 'his',\n",
       " 'more',\n",
       " 'one',\n",
       " 'about',\n",
       " 'their',\n",
       " \"'t\",\n",
       " 'so',\n",
       " 'which',\n",
       " 'It',\n",
       " 'out',\n",
       " 'up',\n",
       " '...',\n",
       " 'were',\n",
       " 'had',\n",
       " 'who',\n",
       " 'like',\n",
       " ';',\n",
       " '“',\n",
       " 'our',\n",
       " 'would',\n",
       " '”',\n",
       " 'time',\n",
       " 'been',\n",
       " 'if',\n",
       " 'also',\n",
       " 'just',\n",
       " 'when',\n",
       " 'her',\n",
       " 'This',\n",
       " 'me',\n",
       " 'there',\n",
       " 'do',\n",
       " 'what',\n",
       " 'some',\n",
       " 'other',\n",
       " 'In',\n",
       " 'them',\n",
       " '–',\n",
       " '1',\n",
       " 'get',\n",
       " 'new',\n",
       " 'into',\n",
       " '&',\n",
       " 'We',\n",
       " 'than',\n",
       " 'A',\n",
       " 'no',\n",
       " 'only',\n",
       " 'first',\n",
       " 'any',\n",
       " 'its',\n",
       " 'people',\n",
       " '2',\n",
       " '$',\n",
       " 'very',\n",
       " 't',\n",
       " 'over',\n",
       " 'she',\n",
       " '%',\n",
       " 'how',\n",
       " 'make',\n",
       " 'You',\n",
       " 'said',\n",
       " 'He',\n",
       " 'two',\n",
       " 'may',\n",
       " 'know',\n",
       " 'then',\n",
       " 'see',\n",
       " 'after',\n",
       " 'most',\n",
       " 'good',\n",
       " 'years',\n",
       " 'If',\n",
       " 'these',\n",
       " 'now',\n",
       " '3',\n",
       " 'use',\n",
       " 'because',\n",
       " 'well',\n",
       " 'work',\n",
       " 'could',\n",
       " 'us',\n",
       " 'don',\n",
       " 'way',\n",
       " 'much',\n",
       " 'back',\n",
       " 'many',\n",
       " 'think',\n",
       " 'where',\n",
       " 'even',\n",
       " 'him',\n",
       " 'through',\n",
       " 'am',\n",
       " '10',\n",
       " '|',\n",
       " 'here',\n",
       " '#',\n",
       " 'made',\n",
       " 'year',\n",
       " 'should',\n",
       " '*',\n",
       " 'really',\n",
       " 'being',\n",
       " 'such',\n",
       " 'need',\n",
       " 'great',\n",
       " 'And',\n",
       " ']',\n",
       " '4',\n",
       " '[',\n",
       " '5',\n",
       " 'day',\n",
       " 'before',\n",
       " 'want',\n",
       " 'used',\n",
       " 'go',\n",
       " 'those',\n",
       " '…',\n",
       " 'But',\n",
       " 'right',\n",
       " \"'m\",\n",
       " 'take',\n",
       " '—',\n",
       " 'May',\n",
       " 'still',\n",
       " 'last',\n",
       " 'off',\n",
       " 'too',\n",
       " 'New',\n",
       " 'going',\n",
       " 'best',\n",
       " 'find',\n",
       " 'love',\n",
       " 'did',\n",
       " 'while',\n",
       " 'home',\n",
       " 'There',\n",
       " 'They',\n",
       " 'same',\n",
       " 'around',\n",
       " 'help',\n",
       " 'down',\n",
       " 'information',\n",
       " 'UTC',\n",
       " 'place',\n",
       " 'i',\n",
       " '2017',\n",
       " 'For',\n",
       " 'little',\n",
       " 'life',\n",
       " 'between',\n",
       " 'each',\n",
       " 'own',\n",
       " 'both',\n",
       " '12',\n",
       " '6',\n",
       " 'world',\n",
       " 'part',\n",
       " 'few',\n",
       " '8',\n",
       " '7',\n",
       " 'talk',\n",
       " 'As',\n",
       " 'look',\n",
       " '2012',\n",
       " 'things',\n",
       " '11',\n",
       " 'say',\n",
       " 'does',\n",
       " 'every',\n",
       " 'something',\n",
       " '2013',\n",
       " 'during',\n",
       " 'got',\n",
       " 'So',\n",
       " \"'ve\",\n",
       " 'What',\n",
       " 'since',\n",
       " 'found',\n",
       " 'long',\n",
       " 'different',\n",
       " 'says',\n",
       " '>',\n",
       " 'never',\n",
       " 'another',\n",
       " '�',\n",
       " 'Ã',\n",
       " 'better',\n",
       " '2016',\n",
       " 'using',\n",
       " '+',\n",
       " 'free',\n",
       " '20',\n",
       " 'under',\n",
       " 'three',\n",
       " 'family',\n",
       " 'She',\n",
       " 'including',\n",
       " 'That',\n",
       " 'always',\n",
       " '\\\\',\n",
       " 'next',\n",
       " 'come',\n",
       " 'without',\n",
       " 'My',\n",
       " 'again',\n",
       " '9',\n",
       " 'game',\n",
       " \"'re\",\n",
       " '15',\n",
       " '2011',\n",
       " 'When',\n",
       " 'days',\n",
       " 'set',\n",
       " '30',\n",
       " 'All',\n",
       " 'number',\n",
       " '2014',\n",
       " 'end',\n",
       " 'lot',\n",
       " 'business',\n",
       " 'sure',\n",
       " 'system',\n",
       " 'book',\n",
       " '2015',\n",
       " 'against',\n",
       " 'high',\n",
       " '=',\n",
       " '2010',\n",
       " 'must',\n",
       " 'available',\n",
       " 'To',\n",
       " 'might',\n",
       " 'show',\n",
       " 'area',\n",
       " 'Â',\n",
       " 'week',\n",
       " '00',\n",
       " 'away',\n",
       " 'team',\n",
       " 'March',\n",
       " 'name',\n",
       " 'until',\n",
       " 'April',\n",
       " 'give',\n",
       " 'thing',\n",
       " 'read',\n",
       " 'put',\n",
       " \"'ll\",\n",
       " 'On',\n",
       " 'small',\n",
       " 'school',\n",
       " 'feel',\n",
       " 'second',\n",
       " 'company',\n",
       " 'June',\n",
       " 'old',\n",
       " 'didn',\n",
       " 'page',\n",
       " 'keep',\n",
       " 'top',\n",
       " 'why',\n",
       " 'January',\n",
       " 'site',\n",
       " '18',\n",
       " 'post',\n",
       " 'today',\n",
       " '16',\n",
       " 'within',\n",
       " '0',\n",
       " 'service',\n",
       " 'having',\n",
       " 'looking',\n",
       " 'American',\n",
       " 'point',\n",
       " 'data',\n",
       " 'though',\n",
       " '½',\n",
       " 'July',\n",
       " 'University',\n",
       " 'full',\n",
       " '14',\n",
       " 'm',\n",
       " 'able',\n",
       " 'left',\n",
       " 'With',\n",
       " 'support',\n",
       " 'United',\n",
       " '2009',\n",
       " '13',\n",
       " 'experience',\n",
       " 'room',\n",
       " '¿',\n",
       " 'water',\n",
       " 'state',\n",
       " '‘',\n",
       " 'big',\n",
       " 'order',\n",
       " 'October',\n",
       " 'No',\n",
       " 'article',\n",
       " 'case',\n",
       " 'making',\n",
       " 'enough',\n",
       " 'ago',\n",
       " 'house',\n",
       " 'September',\n",
       " '17',\n",
       " 'children',\n",
       " 'One',\n",
       " 'local',\n",
       " 'December',\n",
       " 'start',\n",
       " 'times',\n",
       " 'February',\n",
       " 'called',\n",
       " 'August',\n",
       " '25',\n",
       " 'November',\n",
       " 'ever',\n",
       " 'More',\n",
       " '2008',\n",
       " 'done',\n",
       " 'change',\n",
       " 'play',\n",
       " 'group',\n",
       " 'working',\n",
       " 'based',\n",
       " 'online',\n",
       " 'http',\n",
       " 'course',\n",
       " 'least',\n",
       " 'doesn',\n",
       " 'money',\n",
       " 'won',\n",
       " 'man',\n",
       " 'less',\n",
       " '@',\n",
       " 'important',\n",
       " 'After',\n",
       " '24',\n",
       " 'How',\n",
       " '22',\n",
       " 'try',\n",
       " 'getting',\n",
       " 'thought',\n",
       " 'public',\n",
       " 'actually',\n",
       " 'already',\n",
       " 'night',\n",
       " 'person',\n",
       " 're',\n",
       " '21',\n",
       " 'went',\n",
       " '..',\n",
       " 'story',\n",
       " 'several',\n",
       " '--',\n",
       " 'later',\n",
       " 'makes',\n",
       " 'side',\n",
       " 'list',\n",
       " 'following',\n",
       " '19',\n",
       " 'came',\n",
       " 'By',\n",
       " 'doing',\n",
       " 'power',\n",
       " 'large',\n",
       " 'season',\n",
       " 'provide',\n",
       " 'website',\n",
       " 'bit',\n",
       " 'far',\n",
       " 'real',\n",
       " 'known',\n",
       " 'took',\n",
       " 'PM',\n",
       " 'city',\n",
       " '23',\n",
       " 'let',\n",
       " 'At',\n",
       " 'along',\n",
       " 'together',\n",
       " 'per',\n",
       " '}',\n",
       " 'often',\n",
       " 'These',\n",
       " 'music',\n",
       " 'hard',\n",
       " 'God',\n",
       " 'share',\n",
       " 'Your',\n",
       " 'Our',\n",
       " 'line',\n",
       " 'process',\n",
       " 'care',\n",
       " 'others',\n",
       " 'open',\n",
       " 'John',\n",
       " 'car',\n",
       " 'services',\n",
       " 'include',\n",
       " 'food',\n",
       " 'started',\n",
       " 'easy',\n",
       " 'country',\n",
       " 'months',\n",
       " 'fact',\n",
       " 'women',\n",
       " 'yet',\n",
       " 'students',\n",
       " '2007',\n",
       " 'someone',\n",
       " 'once',\n",
       " 'live',\n",
       " 'problem',\n",
       " 'run',\n",
       " 'video',\n",
       " 've',\n",
       " 'become',\n",
       " 'government',\n",
       " 'everything',\n",
       " 'series',\n",
       " 'However',\n",
       " 'anything',\n",
       " 'four',\n",
       " 'pm',\n",
       " 'means',\n",
       " 'stay',\n",
       " '100',\n",
       " 'early',\n",
       " 'possible',\n",
       " 'past',\n",
       " 'design',\n",
       " 'quality',\n",
       " 'City',\n",
       " 'seen',\n",
       " 'States',\n",
       " 'given',\n",
       " 'pretty',\n",
       " 'State',\n",
       " 'blog',\n",
       " 'U.S.',\n",
       " \"'d\",\n",
       " 'film',\n",
       " 'nice',\n",
       " 'history',\n",
       " 'job',\n",
       " 'call',\n",
       " 'Now',\n",
       " 'kind',\n",
       " 'believe',\n",
       " 'community',\n",
       " 'needs',\n",
       " 'level',\n",
       " 'program',\n",
       " 'body',\n",
       " 'view',\n",
       " 'friends',\n",
       " 'control',\n",
       " 'comes',\n",
       " 'York',\n",
       " 'products',\n",
       " 'form',\n",
       " 'above',\n",
       " '26',\n",
       " 'School',\n",
       " 'health',\n",
       " 'minutes',\n",
       " 'probably',\n",
       " 'World',\n",
       " 'hours',\n",
       " 'fun',\n",
       " 'National',\n",
       " 'US',\n",
       " 'either',\n",
       " 'County',\n",
       " 'head',\n",
       " '28',\n",
       " 'example',\n",
       " 'product',\n",
       " 'please',\n",
       " 'close',\n",
       " 'beautiful',\n",
       " 'market',\n",
       " 'across',\n",
       " 'whole',\n",
       " 'offer',\n",
       " 'quite',\n",
       " 'trying',\n",
       " 'price',\n",
       " 'told',\n",
       " 'idea',\n",
       " 'members',\n",
       " 'light',\n",
       " '27',\n",
       " 'million',\n",
       " 'single',\n",
       " 'His',\n",
       " 'perfect',\n",
       " 'Please',\n",
       " 'hope',\n",
       " 'access',\n",
       " 'research',\n",
       " 'due',\n",
       " '2006',\n",
       " 'tell',\n",
       " 'seems',\n",
       " 'South',\n",
       " 'added',\n",
       " 'almost',\n",
       " 'current',\n",
       " 'short',\n",
       " 'bad',\n",
       " 'Not',\n",
       " '50',\n",
       " 'games',\n",
       " 'project',\n",
       " 'review',\n",
       " 'hand',\n",
       " 'email',\n",
       " 'near',\n",
       " 'below',\n",
       " 'men',\n",
       " 'wanted',\n",
       " '29',\n",
       " 'create',\n",
       " 'nothing',\n",
       " 'question',\n",
       " 'de',\n",
       " 'special',\n",
       " 'everyone',\n",
       " 'content',\n",
       " 'Posted',\n",
       " 'future',\n",
       " 'rather',\n",
       " 'check',\n",
       " 'taking',\n",
       " 'Thanks',\n",
       " 'works',\n",
       " 'From',\n",
       " 'month',\n",
       " 'front',\n",
       " 'law',\n",
       " 'space',\n",
       " 'news',\n",
       " 'Here',\n",
       " 'reason',\n",
       " 'add',\n",
       " 'isn',\n",
       " 'anyone',\n",
       " 'report',\n",
       " 'whether',\n",
       " 'major',\n",
       " 'main',\n",
       " 'young',\n",
       " 'development',\n",
       " 'North',\n",
       " 'event',\n",
       " '•',\n",
       " 'results',\n",
       " 'personal',\n",
       " 'especially',\n",
       " 'mean',\n",
       " 'issue',\n",
       " 'five',\n",
       " 'social',\n",
       " 'original',\n",
       " 'age',\n",
       " '....',\n",
       " 'born',\n",
       " 'taken',\n",
       " 'living',\n",
       " 'buy',\n",
       " 'd',\n",
       " 'll',\n",
       " 'Reply',\n",
       " 'reading',\n",
       " 'English',\n",
       " 'mind',\n",
       " 'study',\n",
       " 'building',\n",
       " 'result',\n",
       " 'type',\n",
       " 'looks',\n",
       " 'located',\n",
       " 'features',\n",
       " 'words',\n",
       " 'party',\n",
       " 'needed',\n",
       " 'size',\n",
       " 'plan',\n",
       " 'issues',\n",
       " 'else',\n",
       " 'search',\n",
       " 'black',\n",
       " 'version',\n",
       " 'News',\n",
       " 'included',\n",
       " 'Read',\n",
       " 'books',\n",
       " 'House',\n",
       " 'provided',\n",
       " 'Just',\n",
       " 'visit',\n",
       " 'couple',\n",
       " 'white',\n",
       " 'present',\n",
       " 'link',\n",
       " 'pay',\n",
       " 'coming',\n",
       " 'Some',\n",
       " 'face',\n",
       " 'kids',\n",
       " 'questions',\n",
       " 'friend',\n",
       " 'understand',\n",
       " 'however',\n",
       " 'human',\n",
       " 'became',\n",
       " 'ï',\n",
       " 'value',\n",
       " 'move',\n",
       " 'further',\n",
       " 'member',\n",
       " 'among',\n",
       " 'half',\n",
       " 'weeks',\n",
       " 'third',\n",
       " 'media',\n",
       " 'true',\n",
       " 'America',\n",
       " '»',\n",
       " 'soon',\n",
       " 'range',\n",
       " 'received',\n",
       " 'While',\n",
       " 'points',\n",
       " 'held',\n",
       " 'image',\n",
       " 'title',\n",
       " 'property',\n",
       " 'town',\n",
       " 'shows',\n",
       " 'offers',\n",
       " 'movie',\n",
       " 'happy',\n",
       " 'asked',\n",
       " 'outside',\n",
       " '£',\n",
       " 'various',\n",
       " 'writing',\n",
       " 'etc',\n",
       " 'class',\n",
       " '31',\n",
       " 'Is',\n",
       " 'contact',\n",
       " 'TV',\n",
       " 'upon',\n",
       " 'An',\n",
       " 'style',\n",
       " 'phone',\n",
       " 'stop',\n",
       " 'created',\n",
       " 'location',\n",
       " 'Click',\n",
       " 'wrote',\n",
       " 'running',\n",
       " 'art',\n",
       " 'low',\n",
       " 'child',\n",
       " 'heart',\n",
       " 'date',\n",
       " 'Do',\n",
       " 'matter',\n",
       " 'simple',\n",
       " 'address',\n",
       " 'played',\n",
       " 'similar',\n",
       " 'simply',\n",
       " 'office',\n",
       " 'enjoy',\n",
       " 'myself',\n",
       " 'saw',\n",
       " 'behind',\n",
       " 'complete',\n",
       " 'Center',\n",
       " 'card',\n",
       " 'death',\n",
       " 'problems',\n",
       " 'cannot',\n",
       " 'performance',\n",
       " 'Also',\n",
       " 'turn',\n",
       " 'action',\n",
       " 'learn',\n",
       " 'via',\n",
       " 'general',\n",
       " 'deal',\n",
       " 'includes',\n",
       " 'cost',\n",
       " 'bring',\n",
       " 'written',\n",
       " 'takes',\n",
       " 'window',\n",
       " 'Of',\n",
       " 'former',\n",
       " 'First',\n",
       " 'air',\n",
       " 'leave',\n",
       " 'likely',\n",
       " 'field',\n",
       " 'lost',\n",
       " 'areas',\n",
       " 'private',\n",
       " 'industry',\n",
       " 'store',\n",
       " 'usually',\n",
       " 'late',\n",
       " '2005',\n",
       " 'lead',\n",
       " 'Then',\n",
       " '40',\n",
       " 'clear',\n",
       " 'win',\n",
       " 'total',\n",
       " 'West',\n",
       " 'common',\n",
       " 'morning',\n",
       " 'Home',\n",
       " 'events',\n",
       " 'Day',\n",
       " 'released',\n",
       " 'Google',\n",
       " 'position',\n",
       " 'return',\n",
       " 'required',\n",
       " 'subject',\n",
       " 'account',\n",
       " 'ask',\n",
       " 'gets',\n",
       " 'comment',\n",
       " 'companies',\n",
       " 'provides',\n",
       " 'published',\n",
       " 'energy',\n",
       " 'currently',\n",
       " 'clean',\n",
       " 'President',\n",
       " 'instead',\n",
       " 'comments',\n",
       " 'final',\n",
       " 'longer',\n",
       " 'training',\n",
       " 'playing',\n",
       " '¯',\n",
       " 'period',\n",
       " 'interest',\n",
       " 'word',\n",
       " 'amount',\n",
       " 'source',\n",
       " 'rest',\n",
       " 'record',\n",
       " 'worked',\n",
       " 'Great',\n",
       " 'model',\n",
       " 'addition',\n",
       " 'technology',\n",
       " 'wasn',\n",
       " '<',\n",
       " 'web',\n",
       " 'details',\n",
       " 'role',\n",
       " 'College',\n",
       " 'goes',\n",
       " 'Well',\n",
       " 'Park',\n",
       " 'Free',\n",
       " 'began',\n",
       " 'professional',\n",
       " 'changes',\n",
       " 'strong',\n",
       " 'meet',\n",
       " 'watch',\n",
       " 'hotel',\n",
       " 'certain',\n",
       " 'saying',\n",
       " 'sense',\n",
       " 'forward',\n",
       " 'gave',\n",
       " 'color',\n",
       " 'recently',\n",
       " 'inside',\n",
       " 'wrong',\n",
       " 'album',\n",
       " 'staff',\n",
       " 'designed',\n",
       " 'paper',\n",
       " 'amazing',\n",
       " 'drive',\n",
       " 'woman',\n",
       " 'hit',\n",
       " 'user',\n",
       " 'rights',\n",
       " 'decided',\n",
       " 'recent',\n",
       " 'key',\n",
       " 'continue',\n",
       " 'rate',\n",
       " 'posted',\n",
       " 'itself',\n",
       " 'code',\n",
       " 'favorite',\n",
       " 'seem',\n",
       " 'International',\n",
       " 'political',\n",
       " 'View',\n",
       " 'Thank',\n",
       " 'maybe',\n",
       " 'percent',\n",
       " 'remember',\n",
       " 'Why',\n",
       " '~',\n",
       " 'London',\n",
       " 'test',\n",
       " 'stuff',\n",
       " 'write',\n",
       " 'computer',\n",
       " 'cause',\n",
       " 'cut',\n",
       " 'according',\n",
       " 'built',\n",
       " 'player',\n",
       " 'popular',\n",
       " 'High',\n",
       " 'Most',\n",
       " '01',\n",
       " 'management',\n",
       " 'interesting',\n",
       " 'son',\n",
       " 'ways',\n",
       " 'education',\n",
       " 'sound',\n",
       " 'summer',\n",
       " 'ready',\n",
       " 'natural',\n",
       " 'national',\n",
       " 'software',\n",
       " '·',\n",
       " 'David',\n",
       " 'tried',\n",
       " 'yourself',\n",
       " 'players',\n",
       " 'hot',\n",
       " 'higher',\n",
       " 'policy',\n",
       " 'related',\n",
       " 'Don',\n",
       " 'miles',\n",
       " 'entire',\n",
       " 'specific',\n",
       " 'wonderful',\n",
       " 'Yes',\n",
       " 'felt',\n",
       " 'section',\n",
       " 'British',\n",
       " 'thinking',\n",
       " 'x',\n",
       " 'release',\n",
       " 'song',\n",
       " 'San',\n",
       " 'six',\n",
       " 'heard',\n",
       " 'particular',\n",
       " 'users',\n",
       " 'War',\n",
       " 'average',\n",
       " 'hear',\n",
       " 'Best',\n",
       " 'themselves',\n",
       " 'security',\n",
       " 'file',\n",
       " 'https',\n",
       " 'ones',\n",
       " '05',\n",
       " 'oil',\n",
       " 'Dr.',\n",
       " 'Facebook',\n",
       " 'See',\n",
       " 'California',\n",
       " 'cover',\n",
       " 'allow',\n",
       " 'center',\n",
       " 'Friday',\n",
       " 'terms',\n",
       " 'road',\n",
       " 'receive',\n",
       " 'board',\n",
       " 'war',\n",
       " 'material',\n",
       " 'wife',\n",
       " 'lives',\n",
       " 'sometimes',\n",
       " 'parts',\n",
       " 'UK',\n",
       " 'individual',\n",
       " 'chance',\n",
       " 'Health',\n",
       " 'items',\n",
       " 'answer',\n",
       " 'girl',\n",
       " 'himself',\n",
       " 'India',\n",
       " 'Street',\n",
       " 'walk',\n",
       " 'definitely',\n",
       " 'C',\n",
       " 'production',\n",
       " 'worth',\n",
       " 'systems',\n",
       " 'character',\n",
       " 'throughout',\n",
       " 'reviews',\n",
       " 'giving',\n",
       " 'career',\n",
       " 'increase',\n",
       " 'unique',\n",
       " 'mother',\n",
       " 'completely',\n",
       " 'picture',\n",
       " 'although',\n",
       " 'St.',\n",
       " 'additional',\n",
       " 'customers',\n",
       " 'sex',\n",
       " 'choose',\n",
       " 'parents',\n",
       " '09',\n",
       " 'involved',\n",
       " 'medical',\n",
       " 'follow',\n",
       " 'piece',\n",
       " 'articles',\n",
       " 'red',\n",
       " 'Washington',\n",
       " 'band',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.get_word_vector(\"the\")\n",
    "w2v.get_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = fasttext.load_model(r'C:\\Users\\Joan\\Desktop\\Deep_Learning_project\\cc.en.300.bin')\n",
    "w2v.get_word_vector(\"hello\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01266361, -0.00630743,  0.02671546,  0.02350268,  0.00387836,\n",
       "        0.01581115,  0.00244299,  0.00364108, -0.01176674, -0.01690125,\n",
       "        0.0257959 ,  0.03895067,  0.00998059,  0.03325538, -0.03364278,\n",
       "        0.04455068,  0.06499621,  0.04141721, -0.03342701,  0.02959063,\n",
       "       -0.01507143,  0.00844351, -0.00959196, -0.0140353 , -0.03142786,\n",
       "        0.00042007, -0.01535632,  0.04159728, -0.01703571,  0.04589106,\n",
       "        0.01511274,  0.04722301,  0.00077704,  0.01697644, -0.00581659,\n",
       "       -0.04077702,  0.00131439, -0.04094177, -0.0005262 ,  0.08135634,\n",
       "       -0.0335932 ,  0.00956817, -0.02361193,  0.00249898, -0.05823928,\n",
       "       -0.00178024,  0.01559853,  0.00930665,  0.03453662,  0.05987055,\n",
       "        0.02015573,  0.04371571,  0.0204255 , -0.02030749,  0.01569634,\n",
       "        0.03416956,  0.01384354,  0.02814875, -0.04658603,  0.01151984,\n",
       "       -0.01883863, -0.03550943, -0.04964474, -0.01334785, -0.00716231,\n",
       "       -0.03262212, -0.03730198,  0.0203042 , -0.05168163,  0.03358157,\n",
       "        0.03448504,  0.01319088,  0.04056097,  0.05296937, -0.00385535,\n",
       "        0.01269473,  0.00109657,  0.00471163,  0.01754397,  0.00876622,\n",
       "        0.0271662 ,  0.00719841, -0.0615564 , -0.02157817, -0.00326778,\n",
       "        0.00323567, -0.03242467,  0.00307145, -0.00717416, -0.02360384,\n",
       "       -0.03352971, -0.03351963,  0.02874239,  0.04560966, -0.02235793,\n",
       "       -0.02166793,  0.00281413,  0.01170771, -0.03415733, -0.02065503,\n",
       "       -0.00168927,  0.00335286, -0.01603831, -0.02539855, -0.02226621,\n",
       "       -0.03148664,  0.0118419 , -0.03146388, -0.03284973, -0.00283579,\n",
       "       -0.06238208,  0.04262564, -0.00587674, -0.03257332, -0.0197929 ,\n",
       "       -0.04471276, -0.04473449, -0.01577944,  0.01027391,  0.00906063,\n",
       "        0.01025145,  0.01056307,  0.01238272,  0.0488065 , -0.01244241,\n",
       "       -0.01977874,  0.0377475 , -0.00349984,  0.00110732, -0.01188693,\n",
       "        0.00783989,  0.01090184,  0.01550417, -0.02317157,  0.01494173,\n",
       "       -0.02611639, -0.03850026, -0.00254996,  0.01489715,  0.00429856,\n",
       "        0.03874154,  0.02475509, -0.006947  , -0.01087694, -0.02704986,\n",
       "       -0.00234996, -0.03892149, -0.00163734,  0.00348626, -0.01597354,\n",
       "        0.00187172, -0.02120279,  0.01168853, -0.01375368, -0.04607692,\n",
       "       -0.00468335, -0.01596917,  0.01172939,  0.0092773 ,  0.00303244,\n",
       "       -0.01974697, -0.03618944,  0.02034073,  0.01370443, -0.0027128 ,\n",
       "        0.04083582, -0.00953117, -0.01791569,  0.01862199,  0.0159773 ,\n",
       "       -0.00252127, -0.01386522,  0.04088197, -0.00951528, -0.02234911,\n",
       "        0.00341446, -0.01064812,  0.010957  , -0.03541877, -0.02422902,\n",
       "        0.03673955, -0.01315093,  0.00826955, -0.0289053 , -0.01940208,\n",
       "       -0.01181903, -0.00740843, -0.00866606,  0.03572176,  0.02923592,\n",
       "        0.00800324, -0.00428841,  0.00510379,  0.00048516,  0.01017738,\n",
       "       -0.00464833,  0.02298752,  0.02293579,  0.01746107, -0.01213893,\n",
       "        0.00749806,  0.02453225, -0.02057808, -0.01151866, -0.02199288,\n",
       "       -0.01667725, -0.00888431,  0.02411073, -0.02707534,  0.00545578,\n",
       "       -0.00979026, -0.00613445, -0.02077566, -0.00163498,  0.0174564 ,\n",
       "        0.00019571, -0.0053735 ,  0.0280388 , -0.00255944, -0.03067397,\n",
       "       -0.00101632, -0.03052377, -0.01308517,  0.03508427,  0.02130414,\n",
       "        0.01748728, -0.04085425,  0.00208942,  0.00235797,  0.01298865,\n",
       "        0.01810339,  0.02598175, -0.02172345, -0.02884515, -0.02011663,\n",
       "        0.04653735, -0.04168347, -0.02729942, -0.01031853,  0.0174229 ,\n",
       "        0.01605706, -0.03193706, -0.00673619, -0.02256473,  0.02053958,\n",
       "        0.02492368, -0.036609  ,  0.0057356 , -0.01028887, -0.00423715,\n",
       "        0.00610771, -0.01213569, -0.05730609, -0.03556967, -0.01676471,\n",
       "       -0.01827672,  0.01263098, -0.01640393,  0.02435297, -0.00145248,\n",
       "       -0.03217763,  0.02526928, -0.04970516,  0.00662908, -0.00430914,\n",
       "       -0.02065735, -0.03315062, -0.00353704, -0.02444991,  0.03248503,\n",
       "       -0.00222527, -0.0146547 ,  0.01590095,  0.02352972,  0.00935676,\n",
       "       -0.02311158,  0.01936293, -0.01305667, -0.05468894,  0.01471863,\n",
       "        0.02664098, -0.02781633, -0.06463264,  0.0697158 , -0.04084999,\n",
       "       -0.04273997, -0.01463138,  0.0021682 , -0.03946771,  0.02619469,\n",
       "        0.02895323,  0.03539672, -0.04657308,  0.014528  , -0.00734644,\n",
       "        0.05390748, -0.04292872,  0.02980509, -0.01586108,  0.02761139],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.get_word_vector(\"dsnkhio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch\n",
    "from einops import rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<class 'torchvision.transforms._presets.ImageClassification'>, crop_size=380, resize_size=384, interpolation=<InterpolationMode.BICUBIC: 'bicubic'>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.models.EfficientNet_B4_Weights.IMAGENET1K_V1.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.efficientnet_b4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1792, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.efficientnet_b4()\n",
    "modules=list(model.children())[:-1]\n",
    "modules\n",
    "model=nn.Sequential(*modules)\n",
    "# Random image of 380x380x3\n",
    "x = torch.randn(2, 3, 380, 380)\n",
    "\n",
    "# model(x).view(-1, 1792, 1).size()\n",
    "torch.flatten(model(x), start_dim=1).view(-1, 1792, 1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_classes, depth_transformer, heads_transformer, dim_fc_transformer):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        full_cnn = torchvision.models.convnext_tiny(weights=\"DEFAULT\")   \n",
    "        modules=list(full_cnn.children())[:-2]\n",
    "        self.feature_extractor=nn.Sequential(*modules)\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.dim_features_feature_extractor = 768\n",
    "        self.n_features_feature_extractor = 49\n",
    "        self.text_features = 300\n",
    "        # Dimension in which the images and text are embedded\n",
    "        self.dim = 350\n",
    "\n",
    "        # Embed for the text and image features\n",
    "        self.cnn_features_embed = nn.Linear(self.n_features_feature_extractor, self.dim)\n",
    "        self.text_features_embed = nn.Linear(self.text_features, self.dim)\n",
    "\n",
    "        # Positional embedding for the image features\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.dim_features_feature_extractor + 1, self.dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.dim))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        # The Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.dim, nhead=heads_transformer, dim_feedforward=dim_fc_transformer, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth_transformer)\n",
    "\n",
    "        # Classification fc\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.dim, 256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, txt):\n",
    "        batch_size = img.shape[0]\n",
    "\n",
    "        image_features = self.feature_extractor(img)\n",
    "        print(image_features.size())\n",
    "        image_features = image_features.reshape(batch_size, self.n_features_feature_extractor, self.dim_features_feature_extractor).permute(0, 2, 1)\n",
    "        image_features = self.cnn_features_embed(image_features) \n",
    "\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, image_features), dim=1)\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        text_features = self.text_features_embed(txt)\n",
    "        x = torch.cat((x, text_features), dim=1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x[:, 0]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4002, -0.0557, -0.4475,  0.4087, -0.2469, -0.7703,  0.1720, -0.5460,\n",
       "         -0.3017, -0.0252, -0.1599,  0.0577, -0.1675,  0.1278,  0.2412,  0.2234,\n",
       "          0.2146, -0.0628, -0.2319,  0.0391, -0.2654, -0.2683,  0.0431,  0.3022,\n",
       "         -0.3157,  0.0569,  0.1610, -0.2194],\n",
       "        [ 0.4653,  0.2245, -0.3757,  0.4913,  0.2200, -0.0430,  0.0553, -0.5782,\n",
       "         -0.1992,  0.2546,  0.1512, -0.1181, -0.6191,  0.4201,  0.0352,  0.0398,\n",
       "         -0.1506,  0.1418,  0.0696, -0.2588,  0.2420, -0.3117, -0.2428,  0.1430,\n",
       "         -0.5008, -0.1803,  0.1910,  0.0842]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(num_classes=28, depth_transformer=4, heads_transformer=5, dim_fc_transformer=300)\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "y = torch.randn(2, 20, 300)\n",
    "model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConTextTransformer(num_classes=28, depth=4, heads=5, dim=300, mlp_dim=300)\n",
    "x = torch.randn(2, 3, 240, 240)\n",
    "y = torch.randn(2, 20, 300)\n",
    "model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConTextTransformer(nn.Module):\n",
    "    def __init__(self, *, num_classes, dim, depth, heads, mlp_dim, channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Visual feature extractor\n",
    "        resnet50 = torchvision.models.resnet50(weights=\"DEFAULT\")\n",
    "        modules=list(resnet50.children())[:-2]\n",
    "        self.resnet50=nn.Sequential(*modules)\n",
    "        for param in self.resnet50.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.num_cnn_features = 64  # 8x8\n",
    "        self.dim_cnn_features = 2048\n",
    "        self.dim_fasttext_features = 300\n",
    "\n",
    "        # Embeddings for the visual and textual features\n",
    "        self.cnn_feature_to_embedding = nn.Linear(self.dim_cnn_features, dim)\n",
    "        self.fasttext_feature_to_embedding = nn.Linear(self.dim_fasttext_features, dim)\n",
    "\n",
    "        # Learnable position embeddings (for the visual features) and CLS token\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_cnn_features + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "        # The Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, batch_first=True)\n",
    "        encoder_norm = nn.LayerNorm(dim)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        # Classification Head (MLP)\n",
    "        self.to_cls_token = nn.Identity()\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(mlp_dim, mlp_dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(mlp_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, txt, mask=None):\n",
    "        x = self.resnet50(img)\n",
    "        x = rearrange(x, 'b d h w -> b (h w) d') # this makes a sequence of 64 visual features\n",
    "        print(x.size())\n",
    "        x = self.cnn_feature_to_embedding(x)\n",
    "        print(x.size())\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
    "        print(cls_tokens.size())\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        print(x.size())\n",
    "        print(self.pos_embedding.size())\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        x2 = self.fasttext_feature_to_embedding(txt.float())\n",
    "        x = torch.cat((x,x2), dim=1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        return self.mlp_head(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataloders\n",
    "\n",
    "\"\"\"\n",
    "model, criterion, optimizer, data_transforms_train = make2(config)\n",
    "data_path = \"C:/Users/Joan/Desktop/Deep_Learning_project/features/data/ImageSets/0\"\n",
    "img_dir = \"C:/Users/Joan/Desktop/Deep_Learning_project/features/data/JPEGImages\"\n",
    "anotation_path= r\"C:\\Users\\Joan\\Desktop\\Deep_Learning_project\\dlnn-project_ia-group_15\\anotations.pkl\"\n",
    "train_img_names, y_train, test_img_names, y_test, val_img_names, y_val = load_labels_and_split(data_path)\n",
    "ocr_data = pd.read_pickle(anotation_path)\n",
    "train_dataset = Dataset_ConText(img_dir, train_img_names, y_train, ocr_data, transform=data_transforms_train)\n",
    "train_loader = make_loader(train_dataset, config.batch_size)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions\n",
    "\"\"\"\n",
    "def make(config, train=True, device=\"cuda\"):\n",
    "    # Make the data and model\n",
    "    data_path = \"C:/Users/Joan/Desktop/Deep_Learning_project/features/data/\"\n",
    "    anotation_path= r\"C:\\Users\\Joan\\Desktop\\Deep_Learning_project\\dlnn-project_ia-group_15\\anotations.pkl\"\n",
    "    input_size = 256\n",
    "    if train:\n",
    "        data_transforms_train = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.RandomResizedCrop(input_size),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        train_df, val_df = make_dataframe(data_path, anotation_path, train=train)\n",
    "        train_dataset = Dataset_ConText(train_df, data_transforms_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
    "        val_dataset = Dataset_ConText(val_df, data_transforms_train)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "        # Make the model\n",
    "        model = ConTextTransformer(num_classes=config.classes, channels=3, dim=256, depth=2, heads=4, mlp_dim=512).to(device)\n",
    "\n",
    "        # Make the loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "        return model, train_loader, val_loader, criterion, optimizer\n",
    "    else:\n",
    "        data_transforms_test = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(input_size),\n",
    "            torchvision.transforms.CenterCrop(input_size),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        test_df = make_dataframe(data_path, anotation_path, train=train)\n",
    "        test_dataset = Dataset_ConText(test_df, data_transforms_train)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
    "        return  test_loader\n",
    "\n",
    "# Adds the columns of two dataframes\n",
    "def merge_data(imagesAndLabels, ocr_data):\n",
    "    data = pd.concat([imagesAndLabels, ocr_data], axis=1, join=\"inner\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# Call this function to get the dataframes of the data, if train is True, it will return the train and validation dataframes,\n",
    "#  if not, it will return the test dataframe\n",
    "def make_dataframe(data_dir, anotation_path, train=True):\n",
    "    sets_dir = data_dir + \"/ImageSets/0\"\n",
    "    train_img_names, y_train, test_img_names, y_test, val_img_names, y_val = load_labels_and_split(sets_dir)\n",
    "    ocr_data = pd.read_pickle(anotation_path)\n",
    "    if train:\n",
    "        train_data = load_images(train_img_names, y_train, data_dir)\n",
    "        val_data = load_images(val_img_names, y_val, data_dir)\n",
    "        train_data = merge_data(train_data, ocr_data)\n",
    "        val_data = merge_data(val_data, ocr_data)\n",
    "        return train_data.iloc[:int(len(train_data.index)/2), :], val_data\n",
    "    else:\n",
    "        test_data = load_images(test_img_names, y_test, data_dir)\n",
    "        test_data = merge_data(test_data, ocr_data)\n",
    "        return test_data\n",
    "\n",
    "\n",
    "# Loads the images and creates a dataframe with the correpondent labels\n",
    "def load_images(img_names, labels, data_dir):\n",
    "    img_dir = data_dir + \"JPEGImages\"\n",
    "\n",
    "    list_img = []\n",
    "    for img_name in img_names:\n",
    "        img = Image.open(os.path.join(img_dir, img_name)).convert('RGB')\n",
    "        list_img.append(torch.tensor(img, dtype=torch.ByteTensor).repeat(3, 1, 1))\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    data[\"img\"] = list_img\n",
    "    data[\"label\"] = labels\n",
    "    data[\"name\"] = img_names\n",
    "    data.set_index(\"name\", inplace=True)\n",
    "    data[\"label\"] = data[\"label\"].astype(int)\n",
    "\n",
    "    return data\n",
    "    \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
