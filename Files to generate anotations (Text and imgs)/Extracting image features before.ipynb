{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "from models.models import *\n",
    "import gensim.downloader as api\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"C:\\Users\\Joan\\Desktop\\Deep_Learning_project\\features\\data\" # Path of the data folder from the dataset\n",
    "anotation_path= r\"C:\\Users\\Joan\\Desktop\\Deep_Learning_project\\dlnn-project_ia-group_15\\anotations_keras.pkl\" # Path of the file with the words spotted in the images\n",
    "img_dir = data_path + r\"\\JPEGImages\"\n",
    "txt_dir = data_path + r\"\\ImageSets\\0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Set device to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations of the data\n",
    "input_size = 224    \n",
    "data_transforms_train = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(236, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "    torchvision.transforms.RandomResizedCrop(input_size),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_transforms_test = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(236, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "        torchvision.transforms.CenterCrop(input_size),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v = api.load('glove-wiki-gigaword-300') # Initialize the embeding\n",
    "\n",
    "# ocr_data = pd.read_pickle(anotation_path) # Open the data with the data of the OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_names, y_train, test_img_names, y_test, val_img_names, y_val = load_labels_and_split(txt_dir) # Load labels and split them into train, test and validation sets, with a 60% train, 20% validation 20% test split \n",
    "# Creating the datasets and the loaders for the train, test and validation\n",
    "# Train\n",
    "train_dataset = Dataset_imgs(img_dir, train_img_names, y_train, transform=data_transforms_train)\n",
    "train_loader = make_loader(train_dataset, 1, shuffle=False) # Its not necessary to shuffle this train set as we are not training any model, we are only extracting features\n",
    "# Test\n",
    "test_dataset = Dataset_imgs(img_dir, test_img_names, y_test, transform=data_transforms_test)\n",
    "test_loader = make_loader(test_dataset, 1)\n",
    "# Validation\n",
    "val_dataset = Dataset_imgs(img_dir, val_img_names, y_val, transform=data_transforms_test)\n",
    "val_loader = make_loader(val_dataset, 1)\n",
    "\n",
    "# Make the model\n",
    "model = Feature_Extractor().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(loader, model):\n",
    "    # Passes the images through the model and returns the features, labels and image names\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    img_names = []\n",
    "    for label, img, img_name in tqdm(loader):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        feature = model(img)\n",
    "        features.append(feature.squeeze().cpu().numpy())\n",
    "        labels.append(int(label.cpu().numpy()))\n",
    "        img_names.append(img_name[0])\n",
    "    return features, labels, img_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the validation data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159d6e6491374b44a393ede77b8134cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4851 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Extracting features from the validation data:\")\n",
    "val_features, val_labels, val_img_names = extract_features(val_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the train data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beeb0435579f41b49d84f1fc67b780e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Extracting features from the train data:\")\n",
    "train_features, train_labels, train_img_names = extract_features(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the test data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177864ff4cb34979bd561df6a00cf5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4851 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Extracting features from the test data:\")\n",
    "test_features, test_labels, test_img_names = extract_features(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\"features\": train_features, \"labels\": train_labels, \"img_names\": train_img_names}\n",
    "val_data = {\"features\": val_features, \"labels\": val_labels, \"img_names\": val_img_names}\n",
    "test_data = {\"features\": test_features, \"labels\": test_labels, \"img_names\": test_img_names}\n",
    "data = {\"train\": train_data, \"val\": val_data, \"test\": test_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the features extracted in a pickle file\n",
    "with open(\"features_extracted.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
