{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearnNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading sklearn-0.0.post5.tar.gz (3.7 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [18 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      \n",
      "      If the previous advice does not cover your use case, feel free to report it at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Maria\\github-classroom\\DCC-UAB\\dlnn-project_ia-group_15\\utils\\utils.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtransforms\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/\"\n",
    "anotation_path= \"anotations.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text_detected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n04443257_8635.jpg</th>\n",
       "      <td>[[[108, 82, 81], [109, 83, 82], [109, 83, 82],...</td>\n",
       "      <td>27</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n03249342_31900.jpg</th>\n",
       "      <td>[[[141, 165, 169], [141, 165, 169], [141, 165,...</td>\n",
       "      <td>18</td>\n",
       "      <td>[~Phosat, @PEN, 06a`]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n03119203_18360.jpg</th>\n",
       "      <td>[[[37, 69, 20], [81, 113, 64], [76, 108, 58], ...</td>\n",
       "      <td>7</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n03901974_13946.jpg</th>\n",
       "      <td>[[[253, 253, 253], [252, 252, 252], [251, 251,...</td>\n",
       "      <td>16</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n03871371_14832.jpg</th>\n",
       "      <td>[[[196, 181, 142], [195, 180, 141], [194, 179,...</td>\n",
       "      <td>15</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   img  label   \n",
       "n04443257_8635.jpg   [[[108, 82, 81], [109, 83, 82], [109, 83, 82],...     27  \\\n",
       "n03249342_31900.jpg  [[[141, 165, 169], [141, 165, 169], [141, 165,...     18   \n",
       "n03119203_18360.jpg  [[[37, 69, 20], [81, 113, 64], [76, 108, 58], ...      7   \n",
       "n03901974_13946.jpg  [[[253, 253, 253], [252, 252, 252], [251, 251,...     16   \n",
       "n03871371_14832.jpg  [[[196, 181, 142], [195, 180, 141], [194, 179,...     15   \n",
       "\n",
       "                             text_detected  \n",
       "n04443257_8635.jpg                      []  \n",
       "n03249342_31900.jpg  [~Phosat, @PEN, 06a`]  \n",
       "n03119203_18360.jpg                     []  \n",
       "n03901974_13946.jpg                     []  \n",
       "n03871371_14832.jpg                     []  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = make_dataframe(data_path, anotation_path, train=False)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text_detected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n03039827_16253.jpg</th>\n",
       "      <td>[[[130, 134, 143], [130, 134, 143], [130, 134,...</td>\n",
       "      <td>10</td>\n",
       "      <td>[She, KLEEN-It, @ny Cleanehs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n03039827_10498.jpg</th>\n",
       "      <td>[[[255, 255, 255], [255, 255, 255], [255, 255,...</td>\n",
       "      <td>10</td>\n",
       "      <td>[@RY CL_ANING, F4L, Curana2 4T7ee31, hout]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n03039827_8430.jpg</th>\n",
       "      <td>[[[161, 178, 208], [162, 179, 209], [163, 180,...</td>\n",
       "      <td>10</td>\n",
       "      <td>[1, Sisvie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n03039827_3781.jpg</th>\n",
       "      <td>[[[42, 72, 132], [41, 71, 131], [41, 71, 131],...</td>\n",
       "      <td>10</td>\n",
       "      <td>[Tnina, IL, didy, cleincR6, DRTVE, Tn, EiG, In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n03039827_16136.jpg</th>\n",
       "      <td>[[[155, 164, 181], [156, 165, 182], [157, 166,...</td>\n",
       "      <td>10</td>\n",
       "      <td>[1, CLEANERS, SAME, DAY, SERVICE]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   img  label   \n",
       "n03039827_16253.jpg  [[[130, 134, 143], [130, 134, 143], [130, 134,...     10  \\\n",
       "n03039827_10498.jpg  [[[255, 255, 255], [255, 255, 255], [255, 255,...     10   \n",
       "n03039827_8430.jpg   [[[161, 178, 208], [162, 179, 209], [163, 180,...     10   \n",
       "n03039827_3781.jpg   [[[42, 72, 132], [41, 71, 131], [41, 71, 131],...     10   \n",
       "n03039827_16136.jpg  [[[155, 164, 181], [156, 165, 182], [157, 166,...     10   \n",
       "\n",
       "                                                         text_detected  \n",
       "n03039827_16253.jpg                      [She, KLEEN-It, @ny Cleanehs]  \n",
       "n03039827_10498.jpg         [@RY CL_ANING, F4L, Curana2 4T7ee31, hout]  \n",
       "n03039827_8430.jpg                                         [1, Sisvie]  \n",
       "n03039827_3781.jpg   [Tnina, IL, didy, cleincR6, DRTVE, Tn, EiG, In...  \n",
       "n03039827_16136.jpg                  [1, CLEANERS, SAME, DAY, SERVICE]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, val = make_dataframe(data_path, anotation_path, train=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text_detected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n02871525_16645.jpg</th>\n",
       "      <td>[[[254, 254, 252], [254, 254, 252], [255, 255,...</td>\n",
       "      <td>4</td>\n",
       "      <td>[8t, Wvanan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n04081281_14002.jpg</th>\n",
       "      <td>[[[116, 129, 112], [204, 215, 198], [183, 192,...</td>\n",
       "      <td>21</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n02776631_16167.jpg</th>\n",
       "      <td>[[[126, 110, 84], [126, 110, 84], [127, 111, 8...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n04081281_32798.jpg</th>\n",
       "      <td>[[[6, 8, 7], [6, 8, 7], [6, 8, 7], [6, 8, 7], ...</td>\n",
       "      <td>21</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n04398497_39377.jpg</th>\n",
       "      <td>[[[249, 253, 255], [244, 248, 255], [158, 162,...</td>\n",
       "      <td>25</td>\n",
       "      <td>[TEA, roVSrENg$ , \"HSHENDS]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   img  label   \n",
       "n02871525_16645.jpg  [[[254, 254, 252], [254, 254, 252], [255, 255,...      4  \\\n",
       "n04081281_14002.jpg  [[[116, 129, 112], [204, 215, 198], [183, 192,...     21   \n",
       "n02776631_16167.jpg  [[[126, 110, 84], [126, 110, 84], [127, 111, 8...      1   \n",
       "n04081281_32798.jpg  [[[6, 8, 7], [6, 8, 7], [6, 8, 7], [6, 8, 7], ...     21   \n",
       "n04398497_39377.jpg  [[[249, 253, 255], [244, 248, 255], [158, 162,...     25   \n",
       "\n",
       "                                   text_detected  \n",
       "n02871525_16645.jpg                 [8t, Wvanan]  \n",
       "n04081281_14002.jpg                           []  \n",
       "n02776631_16167.jpg                           []  \n",
       "n04081281_32798.jpg                           []  \n",
       "n04398497_39377.jpg  [TEA, roVSrENg$ , \"HSHENDS]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time,os,json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "# fasttext library gives errors when installing\n",
    "# import fasttext\n",
    "# import fasttext.util\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "2.0.1+cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConTextDataset(Dataset):\n",
    "    def __init__(self, json_file, root_dir, root_dir_txt, train=True, transform=None):\n",
    "        with open(json_file) as f:\n",
    "            data = json.load(f)\n",
    "        self.train = train\n",
    "        self.root_dir = root_dir\n",
    "        self.root_dir_txt = root_dir_txt\n",
    "        self.transform = transform\n",
    "        if (self.train):\n",
    "            self.samples = data['train']\n",
    "        else:\n",
    "            self.samples = data['test']\n",
    "\n",
    "        fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "        self.fasttext = fasttext.load_model('cc.en.300.bin')\n",
    "        self.dim_fasttext = self.fasttext.get_dimension()\n",
    "        self.max_num_words = 64\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_name = os.path.join(self.root_dir, self.samples[idx][0]+'.jpg')\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        text = np.zeros((self.max_num_words, self.dim_fasttext))\n",
    "        text_mask = np.ones((self.max_num_words,), dtype=bool)\n",
    "        text_name = os.path.join(self.root_dir_txt, self.samples[idx][0]+'.json')\n",
    "        with open(text_name) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        words = []\n",
    "        if 'textAnnotations' in data.keys():\n",
    "            for i in range(1,len(data['textAnnotations'])):\n",
    "                word = data['textAnnotations'][i]['description']\n",
    "                if len(word) > 2: words.append(word)\n",
    "\n",
    "        words = list(set(words))\n",
    "        for i,w in enumerate(words):\n",
    "            if i>=self.max_num_words: break\n",
    "            text[i,:] = self.fasttext.get_word_vector(w)\n",
    "            text_mask[i] = False\n",
    "        \n",
    "        target = self.samples[idx][1] - 1\n",
    "\n",
    "        return image, text, text_mask, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConTextTransformer(nn.Module):\n",
    "    def __init__(self, *, image_size, num_classes, dim, depth, heads, mlp_dim, channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        resnet50 = torchvision.models.resnet50(pretrained=True)\n",
    "        modules=list(resnet50.children())[:-2]\n",
    "        self.resnet50=nn.Sequential(*modules)\n",
    "        for param in self.resnet50.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.num_cnn_features = 64  # 8x8\n",
    "        self.dim_cnn_features = 2048\n",
    "        self.dim_fasttext_features = 300\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_cnn_features + 1, dim))\n",
    "        self.cnn_feature_to_embedding = nn.Linear(self.dim_cnn_features, dim)\n",
    "        self.fasttext_feature_to_embedding = nn.Linear(self.dim_fasttext_features, dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, batch_first=True)\n",
    "        encoder_norm = nn.LayerNorm(dim)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        self.to_cls_token = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, txt, mask=None):\n",
    "        x = self.resnet50(img)\n",
    "        x = rearrange(x, 'b d h w -> b (h w) d')\n",
    "        x = self.cnn_feature_to_embedding(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        x2 = self.fasttext_feature_to_embedding(txt.float())\n",
    "        x = torch.cat((x,x2), dim=1)\n",
    "\n",
    "        #tmp_mask = torch.zeros((img.shape[0], 1+self.num_cnn_features), dtype=torch.bool)\n",
    "        #mask = torch.cat((tmp_mask.to(device), mask), dim=1)\n",
    "        #x = self.transformer(x, src_key_padding_mask=mask)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = '/datatmp/datasets/ConText/annotations/split_0.json'\n",
    "img_dir = \"/datatmp/datasets/ConText/data/JPEGImages/\"\n",
    "txt_dir = \"/datatmp/datasets/ConText/ocr_labels/\"\n",
    "input_size = 256\n",
    "\n",
    "data_transforms_train = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomResizedCrop(input_size),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),  # ???? we don't care about text?\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "data_transforms_test = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(input_size),\n",
    "        torchvision.transforms.CenterCrop(input_size),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "train_set = ConTextDataset(json_file, img_dir, txt_dir, True, data_transforms_train)\n",
    "test_set  = ConTextDataset(json_file, img_dir, txt_dir, False, data_transforms_test)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, data_loader, loss_history):\n",
    "    total_samples = len(data_loader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    # each iteration is a batch of data\n",
    "    for i, (data_img, data_txt, target) in enumerate(data_loader):\n",
    "        data_img = data_img.to(device)\n",
    "        data_txt = data_txt.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # why do we do a log_softmax here?\n",
    "        # because we use NLLLoss, which expects log probabilities as input\n",
    "        output = F.log_softmax(model(data_img, data_txt), dim=1)\n",
    "        # what does the nll_loss do?\n",
    "        # it computes the negative log-likelihood of the output w.r.t. the target\n",
    "        # for instance if the target is 3, and the output is [0.1, 0.2, 0.6, 0.1], the loss is -log(0.6) = 0.51\n",
    "        # why is this better than MSE?\n",
    "        # because it is more robust to outliers, and because it is more natural to optimize a probability\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('[' +  '{:5}'.format(i * len(data_img)) + '/' + '{:5}'.format(total_samples) +\n",
    "                 ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
    "                '{:6.4f}'.format(loss.item()))\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, loss_history):\n",
    "    model.eval()\n",
    "\n",
    "    total_samples = len(data_loader.dataset)\n",
    "    correct_samples = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data_img, data_txt, target in data_loader:\n",
    "            data_img = data_img.to(device)\n",
    "            data_txt = data_txt.to(device)\n",
    "            target = target.to(device)\n",
    "            output = F.log_softmax(model(data_img, data_txt), dim=1)\n",
    "            loss = F.nll_loss(output, target, reduction='sum')\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct_samples += pred.eq(target).sum()\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    loss_history.append(avg_loss)\n",
    "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
    "        '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
    "        '{:5}'.format(total_samples) + ' (' +\n",
    "        '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')\n",
    "\n",
    "    return correct_samples / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14396\\3151374707.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mword_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hello'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1718\u001b[0m         \"\"\"\n\u001b[1;32m-> 1719\u001b[1;33m         return _load_word2vec_format(\n\u001b[0m\u001b[0;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2047\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2048\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2049\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2050\u001b[0m             \u001b[1;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mve\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m     \u001b[0mbinary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open_binary_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m     \u001b[0mdecompressed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mso_compression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_open_binary_stream\u001b[1;34m(uri, mode, transport_params)\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[0mscheme\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sniff_scheme\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[0msubmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_transport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscheme\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m     \u001b[0mfobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mfobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muri\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\smart_open\\local_file.py\u001b[0m in \u001b[0;36mopen_uri\u001b[1;34m(uri_as_string, mode, transport_params)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mopen_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri_as_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mparsed_uri\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri_as_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mfobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'uri_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin.gz'"
     ]
    }
   ],
   "source": [
    "# we need to create an embeding for the text data\n",
    "# we cannot use fasttext, so a solution is to use a pretrained word2vec model like word2vec-google-news-300\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "word_vectors['hello']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[\"label\"]\n",
    "data_img_train = train[\"img\"]\n",
    "data_txt_train = train[\"text_detected\"]\n",
    "\n",
    "y_test = test[\"label\"]\n",
    "data_img_test = test[\"img\"]\n",
    "data_txt_test = test[\"text_detected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader((data_img_train, data_txt_train, y_train), batch_size=64, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader((data_img_test, data_txt_test, y_test), batch_size=64, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "start_time = time.time()\n",
    "\n",
    "model = ConTextTransformer(image_size=input_size, num_classes=28, channels=3, dim=256, depth=2, heads=4, mlp_dim=512)\n",
    "model.to(device)\n",
    "params_to_update = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=0.0001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15,30], gamma=0.1)\n",
    "\n",
    "train_loss_history, test_loss_history = [], []\n",
    "best_acc = 0.\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    print('Epoch:', epoch)\n",
    "    train_epoch(model, optimizer, train_loader, train_loss_history)\n",
    "    acc = evaluate(model, test_loader, test_loss_history)\n",
    "    if acc>best_acc: torch.save(model.state_dict(), 'all_best.pth')\n",
    "    scheduler.step()\n",
    "\n",
    "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
